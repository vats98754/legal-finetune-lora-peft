{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214f37a9",
   "metadata": {},
   "source": [
    "# Fine-tune a small LLM on public legal data with LoRA (Transformers Trainer)\n",
    "\n",
    "This notebook shows a minimal end-to-end fine-tuning using Hugging Face Transformers + PEFT (LoRA/QLoRA) on a tiny slice of public legal text (Pile of Law). It follows the Trainer flow described in the Transformers Training docs:\n",
    "\n",
    "- Docs: https://huggingface.co/docs/transformers/en/training\n",
    "\n",
    "Notes\n",
    "- On Linux with CUDA, this will use 4-bit quantization (QLoRA) if bitsandbytes is available.\n",
    "- On macOS or CPU-only, it falls back to standard LoRA without 4-bit quantization.\n",
    "- For quick runs, we train on a small subset. Increase as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running first time, uncomment to install dependencies\n",
    "# %pip install -U \"transformers>=4.41.0\" datasets peft accelerate bitsandbytes sentencepiece\n",
    "# %pip install -U huggingface_hub\n",
    "\n",
    "import sys, platform\n",
    "print(platform.platform())\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c8256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Basic config\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "DATASET_ID = \"pile-of-law/pile-of-law\"  # public legal corpus\n",
    "OUTPUT_DIR = \"outputs/legal-qlora\"\n",
    "BLOCK_SIZE = 1024\n",
    "TRAIN_DOCS = 3000  # small for demo; increase for better results\n",
    "SEED = 42\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {use_cuda}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d756dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Optional: login for gated models or to push results (not needed for public TinyLlama)\n",
    "# login()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load a small subset of Pile of Law for a quick demo\n",
    "raw_ds = load_dataset(DATASET_ID, split=\"train\", trust_remote_code=True)\n",
    "raw_ds = raw_ds.shuffle(seed=SEED).select(range(min(TRAIN_DOCS, len(raw_ds))))\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], add_special_tokens=False)\n",
    "\n",
    "tokenized = raw_ds.map(tokenize_fn, batched=True, remove_columns=raw_ds.column_names)\n",
    "\n",
    "# Pack into fixed-length sequences for Causal LM\n",
    "BLOCK_SIZE = min(BLOCK_SIZE, tokenizer.model_max_length if tokenizer.model_max_length and tokenizer.model_max_length < 100_000 else BLOCK_SIZE)\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_len = len(concatenated[\"input_ids\"])\n",
    "    total_len = (total_len // BLOCK_SIZE) * BLOCK_SIZE\n",
    "    result = {\n",
    "        k: [t[i:i + BLOCK_SIZE] for i in range(0, total_len, BLOCK_SIZE)]\n",
    "        for k, t in concatenated.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_ds = tokenized.map(group_texts, batched=True)\n",
    "print(lm_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: optionally use 4-bit quantization (QLoRA) when CUDA+bitsandbytes available\n",
    "use_bnb = False\n",
    "try:\n",
    "    import bitsandbytes as bnb  # noqa: F401\n",
    "    use_bnb = use_cuda\n",
    "except Exception:\n",
    "    use_bnb = False\n",
    "\n",
    "bnb_config = None\n",
    "if use_bnb:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,\n",
    "    )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# LoRA target modules for LLaMA-like models\n",
    "peft_cfg = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, peft_cfg)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.0,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    bf16=bf16,\n",
    "    fp16=(not bf16) and torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to=[\"none\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    train_dataset=lm_ds,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(train_result)\n",
    "trainer.save_model(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb12081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inference to verify training worked\n",
    "from transformers import pipeline\n",
    "\n",
    "gen_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "prompt = \"You are a legal assistant. Summarize the following clause: The party of the first part agrees to indemnify and hold harmless...\"\n",
    "output = gen_pipe(prompt, max_new_tokens=128, do_sample=True, top_p=0.9, temperature=0.7)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d31629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adapters only (PEFT) and tokenizer\n",
    "adapter_dir = os.path.join(OUTPUT_DIR, \"lora-adapter\")\n",
    "os.makedirs(adapter_dir, exist_ok=True)\n",
    "model.save_pretrained(adapter_dir)\n",
    "tokenizer.save_pretrained(adapter_dir)\n",
    "print(f\"Saved LoRA adapter to: {adapter_dir}\")\n",
    "\n",
    "# Optionally push to Hub (requires login and write access)\n",
    "# model.push_to_hub(\"<your-username>/tinyllama-legal-lora\")\n",
    "# tokenizer.push_to_hub(\"<your-username>/tinyllama-legal-lora\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
